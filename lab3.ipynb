import os
import re
from tqdm import tqdm
from glob import glob

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import cv2
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
# Directorio donde se encuentran las imágenes
IMAGE_PATH = "data/UTKFACE"

# Regex para capturar las etiquetas de género en el nombre del archivo
# [age]_[gender]_[race]_[date&time].jpg.chip.jpg
FILENAME_PATTERN = re.compile(r"\d+_([01])_[0-4]_\d+.jpg.chip.jpg")
data = []
invalid_file_paths = []

# Filtrar imágenes con etiquetas válidas
try:
    all_files = glob(f"{IMAGE_PATH}/*.jpg")
    if not all_files:
        raise FileNotFoundError(
            f"No se encontraron archivos en el directorio {IMAGE_PATH}"
        )

    for file_path in tqdm(all_files, desc="Filtrando imágenes"):
        filename = os.path.basename(file_path)
        match = FILENAME_PATTERN.match(filename)

        if match:
            [gender] = match.groups()
            data.append(
                {
                    "file_path": filename,
                    "gender": int(gender),
                }
            )
        else:
            invalid_file_paths.append(filename)

    df = pd.DataFrame(data)
    # Eliminar variables temporales para liberar memoria
    del data, all_files

    print("\n¡Filtrado completado con éxito!")

except FileNotFoundError as e:
    print(f"\nError de ruta: {e}")
    df = pd.DataFrame()
except Exception as e:
    print(f"\nError inesperado: {e}")
    df = pd.DataFrame()

if invalid_file_paths:
    print(f"\nImágenes con etiqueta inválida: {len(invalid_file_paths)}")
    for file in invalid_file_paths:
        print(f"- {file}")
else:
    print("\nSe filtraron todas las imágenes correctamente.")
# Dimensión objetivo en píxeles
TARGET_SIZE = (32, 32)


def preprocess_image(filename: str, images_path: str) -> np.ndarray:
    """
    Preprocesa una imagen: redimensiona, convierte a escala de grises y normaliza.

    Args:
        filename: Nombre del archivo de la imagen.
        images_path: Ruta del directorio que contiene las imágenes.

    Returns:
        Vector numpy con la imagen preprocesada.
    """
    file_path = os.path.join(images_path, filename)

    try:
        img = cv2.imread(file_path)

        if img is None:
            raise ValueError(f"Error al cargar la imagen: {file_path}")

        # Redimensionar a tamaño objetivo
        img = cv2.resize(img, TARGET_SIZE)
        # Convertir a escala de grises
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        # Normalizar a rango [0, 1]
        img = img / 255.0

        # Aplanar la imagen en un vector
        vector = img.flatten()

        return vector

    except ValueError as e:
        print(e)
        return None
    except Exception as e:
        print(f"Error inesperado al procesar la imagen: {e}")
        return None
image_vectors = []
valid_indices = []
invalid_images = []

# Preprocesar imágenes y almacenar vectores válidos
for index, row in tqdm(df.iterrows(), total=len(df), desc="Preprocesando imágenes"):
    vector = preprocess_image(row["file_path"], IMAGE_PATH)

    if vector is not None:
        image_vectors.append(vector)
        valid_indices.append(index)
    else:
        invalid_images.append(row["file_path"])

print(f"\n¡Preprocesamiento completado!")

if invalid_images:
    print(f"\nImágenes no procesadas: {len(invalid_images)}")
    for img in invalid_images:
        print(f"- {img}")
else:
    print("\nSe procesaron todas las imágenes correctamente.")
# Crear matriz de datos X y vector de etiquetas y
X = np.array(image_vectors)

df = df.loc[valid_indices].reset_index(drop=True)
y = df["gender"].values

print("¡Datos preparados!")
print(f"\nDimensión de la matriz de datos X: {X.shape}")
print(f"Dimensión del vector de etiquetas y: {y.shape}")

del df, image_vectors, valid_indices
TEST_SIZE = 0.2
VALIDATION_SIZE = 0.2

RANDOM_SEED = 42
# Separar el conjunto de prueba del resto de los datos
X_temp, X_test, y_temp, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED, stratify=y
)

# Separar el conjunto de validación del conjunto de entrenamiento
VALIDATION_SIZE_ADJUSTED = VALIDATION_SIZE / (1 - TEST_SIZE)

X_train, X_val, y_train, y_val = train_test_split(
    X_temp,
    y_temp,
    test_size=VALIDATION_SIZE_ADJUSTED,
    random_state=RANDOM_SEED,
    stratify=y_temp,
)

print("¡División de datos completada!")
print(f"\nDimensiones de los conjuntos:")
print(
    f"- Entrenamiento ({round((1 - TEST_SIZE - VALIDATION_SIZE)*100)}%): X_train: {X_train.shape}, y_train: {y_train.shape}"
)
print(
    f"- Validación ({round(VALIDATION_SIZE*100)}%): X_val: {X_val.shape}, y_val: {y_val.shape}"
)
print(
    f"- Prueba ({round(TEST_SIZE*100)}%): X_test: {X_test.shape}, y_test: {y_test.shape}"
)

print()

del X, y, X_temp, y_temp
# Estandarizar los datos
scaler = StandardScaler()

Z_train = scaler.fit_transform(X_train)
Z_val = scaler.transform(X_val)
Z_test = scaler.transform(X_test)

del X_train, X_val, X_test
# Ajustar PCA sobre el conjunto de entrenamiento
# n_components=None para conservar toda la varianza
pca = PCA(n_components=None)
pca.fit(Z_train)

# Calcular la varianza explicada por cada componente
explained_variance = pca.explained_variance_ratio_
# Calcular la varianza explicada acumulada
cumulative_variance = np.cumsum(explained_variance)

print("Análisis de PCA:")
print(f"\nNúmero de componentes calculados: {len(explained_variance)}")
print("Varianza explicada por los primeros 5 componentes:")
for i in range(5):
    print(f"- [{i+1}]: {explained_variance[i]*100:.2f}%")
plt.figure(figsize=(10, 6))

# Gráfico de varianza explicada por componente
plt.subplot(2, 1, 1)
plt.plot(
    range(1, len(explained_variance) + 1),
    explained_variance,
    marker="o",
    linestyle="--",
)
plt.title("Varianza Explicada por Componente Principal")
plt.xlabel("Número de Componente Principal (j)")
plt.ylabel("Varianza Explicada")
plt.grid(True)

# Gráfico de varianza explicada acumulada
plt.subplot(2, 1, 2)
plt.plot(
    range(1, len(cumulative_variance) + 1),
    cumulative_variance,
    marker="o",
    linestyle="-",
    color="red",
)
plt.title("Varianza Explicada Acumulada")
plt.xlabel("Número de Componente Principal (k)")
plt.ylabel("Varianza Acumulada")

# Identificar punto donde se alcanza el 95% de varianza explicada
V_TARGET = 0.95
k_95 = np.argmax(cumulative_variance >= V_TARGET) + 1

# Dibujar líneas para identificar el 95% y el k óptimo
plt.axhline(
    y=V_TARGET, color="green", linestyle="-", label=f"{V_TARGET*100:.0f}% de Varianza"
)
plt.axvline(x=k_95, color="gray", linestyle="--", label=f"k = {k_95}")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
# Reajustar PCA con el número óptimo de componentes
pca = PCA(n_components=k_95, random_state=RANDOM_SEED)

# Proyectar los datos en el nuevo espacio PCA
Z_train = pca.fit_transform(Z_train)
Z_val = pca.transform(Z_val)
Z_test = pca.transform(Z_test)

print("¡Transformación PCA completada!")

print("\nMatriz de entrenamiento tras PCA (Z_train):", Z_train.shape)
print("Matriz de validación tras PCA (Z_val):", Z_val.shape)
print("Matriz de prueba tras PCA (Z_test):", Z_test.shape)
# Ajustar LDA sobre el conjunto de entrenamiento Z_train
lda = LDA(n_components=1)
lda.fit(Z_train, y_train)

# Obtener el vector de proyección LDA
w_lda = lda.scalings_

print("¡Ajuste de LDA completado!")
print("\nVector de proyección LDA (w_lda):", w_lda.shape)
# Proyectar los datos en el espacio LDA
X_train_lda = lda.transform(Z_train).flatten()
X_val_lda = lda.transform(Z_val).flatten()
X_test_lda = lda.transform(Z_test).flatten()

del Z_train, Z_val, Z_test

print("¡Transformación LDA completada!")

print("\nDatos de entrenamiento tras LDA (X_train_lda):", X_train_lda.shape)
print("Datos de validación tras LDA (X_val_lda):", X_val_lda.shape)
print("Datos de prueba tras LDA (X_test_lda):", X_test_lda.shape)
# Subconjuntos de datos de entreo para cada clase
X_male = X_train_lda[y_train == 0]
X_female = X_train_lda[y_train == 1]

# Estimar parámetros de la distribución normal para cada clase
mu_male = np.mean(X_male)
sigma2_male = np.var(X_male)

mu_female = np.mean(X_female)
sigma2_female = np.var(X_female)

print("Paramétros estimados de la distribución normal:")

print(f"\nClase Masculino (0): μ = {mu_male:.4f}, σ² = {sigma2_male:.4f}")
print(f"Clase Femenino (1): μ = {mu_female:.4f}, σ² = {sigma2_female:.4f}")
def gaussian_pdf(x: np.ndarray, mu: float, sigma2: float) -> np.ndarray:
    """
    Calcula la función de densidad de probabilidad (PDF) de una distribución normal.

    Args:
        x: Datos de entrada.
        mu: Media de la distribución.
        sigma2: Varianza de la distribución.

    Returns:
        Valores de la PDF evaluados en x.
    """
    coeff = 1 / np.sqrt(2 * np.pi * sigma2)
    exponent = -((x - mu) ** 2) / (2 * sigma2)
    return coeff * np.exp(exponent)
# Crear un rango de valores para graficar las distribuciones
x_range = np.linspace(min(X_train_lda) - 0.5, max(X_train_lda) + 0.5, 500)

# Calcular las PDFs para cada clase
pdf_male = gaussian_pdf(x_range, mu_male, sigma2_male)
pdf_female = gaussian_pdf(x_range, mu_female, sigma2_female)

plt.figure(figsize=(10, 6))

# Histogramas normalizados (density=True)
plt.hist(X_male, bins=30, density=True, alpha=0.6, label="Male (Hist.)", color="blue")
plt.hist(
    X_female, bins=30, density=True, alpha=0.6, label="Female (Hist.)", color="red"
)

# Curvas de las densidades Gaussianas estimadas
plt.plot(x_range, pdf_male, "b--", linewidth=2, label=f"p(X|Male) μ={mu_male:.2f}")
plt.plot(
    x_range, pdf_female, "r--", linewidth=2, label=f"p(X|Female) μ={mu_female:.2f}"
)

plt.title("Distribuciones p(X | Clase) sobre la Componente LDA")
plt.xlabel("Proyección Unidimensional X (Espacio LDA)")
plt.ylabel("Densidad de Probabilidad")
plt.legend()
plt.grid(axis="y", alpha=0.5)
plt.show()
# Calcular probabilidades a priori
N_train = len(y_train)
N_male = np.sum(y_train == 0)
N_female = np.sum(y_train == 1)

p_male = N_male / N_train
p_female = N_female / N_train

print("Probabilidades a priori:")
print(f"\np(Male): {p_male:.4f}")
print(f"p(Female): {p_female:.4f}")
def posterior_probability(X: np.ndarray) -> np.ndarray:
    """
    Calcula la probabilidad a posteriori para las clases Male y Female para un
    conjunto de datos X.

    Args:
        X: Datos de entrada en el espacio LDA.

    Returns:
        2-tupla con las probabilidades a posteriori para cada clase.
    """
    # Calcular las PDFs para cada clase
    p_x_given_male = gaussian_pdf(X, mu_male, sigma2_male)
    p_x_given_female = gaussian_pdf(X, mu_female, sigma2_female)

    # Calular la probabilidad total p(x)
    p_x = (p_x_given_male * p_male) + (p_x_given_female * p_female)

    # Evitar división por cero
    p_x[p_x == 0] = 1e-10

    # Calcular las probabilidades a posteriori
    p_male_given_x = (p_x_given_male * p_male) / p_x
    p_female_given_x = (p_x_given_female * p_female) / p_x

    return p_male_given_x, p_female_given_x
# Calcular probabilidades a posteriori para cada conjunto
p_male_train, p_female_train = posterior_probability(X_train_lda)
p_male_test, p_female_test = posterior_probability(X_test_lda)
p_male_val, p_female_val = posterior_probability(X_val_lda)

print("Cálculo de probabilidades a posteriori completado.")
print(f"\nPrimeras 5 probabilidades a posteriori en el conjunto de entrenamiento:")
print(f"Male: {p_male_train[:5].round(4)}")
print(f"Female: {p_female_train[:5].round(4)}")
# Implementar regla de decisión Bayesiana
y_pred_nb_test = (p_male_test > 0.5).astype(int)

print("--- Predicciones Naive Bayes ---\n")

print(f"Etiquetas reales y_test: {y_test[:10]}")
print(f"Etiquetas predichas y_pred_nb_test: {y_pred_nb_test[:10]}")
p_male_range, p_female_range = posterior_probability(x_range)

plt.plot(x_range, p_male_range, 'b-', linewidth=2, label='p(Male | x)')
plt.plot(x_range, p_female_range, 'r-', linewidth=2, label='p(Female | x)')

# La frontera de decisión está donde p(Male|x) = p(Female|x) = 0.5
nb_frontier = x_range[np.argmin(np.abs(p_male_range - 0.5))]
plt.axvline(x=nb_frontier, color='k', linestyle='--', label=f'Frontera de Decisión Naive Bayes ({nb_frontier:.2f})')
plt.axhline(y=0.5, color='gray', linestyle=':')

plt.title('Probabilidades Posteriores en el Espacio LDA (Naive Bayes)')
plt.xlabel('Proyección Unidimensional X')
plt.ylabel('Probabilidad Posterior')
plt.legend()
plt.grid(axis='y', alpha=0.5)
plt.show()
